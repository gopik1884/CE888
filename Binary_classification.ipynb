{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2                \n",
    "import numpy as np      \n",
    "import os             \n",
    "from random import shuffle \n",
    "from tqdm import tqdm  \n",
    "from sklearn.metrics import roc_auc_score    \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os, sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "IMG_SIZE = 80\n",
    "\n",
    "epochs = 5\n",
    "step_size = 8\n",
    "IMG_SIZE_ALEXNET = 227\n",
    "validating_size = 40\n",
    "nodes_fc1 = 4096\n",
    "nodes_fc2 = 4096\n",
    "output_classes = 2\n",
    "\n",
    "TRAIN_DIR = os.getcwd()\n",
    "\n",
    "\n",
    "\n",
    "print(TRAIN_DIR) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "with zipfile.ZipFile(\"CT_COVID.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "train_data = np.load(os.path.join(os.getcwd(), 'CT_COVID' ,'train_data_bi.npy'))\n",
    "test_data = np.load(os.path.join(os.getcwd(), 'CT_COVID' ,'test_data_bi.npy'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i][0] = cv2.resize(train_data[i][0],(IMG_SIZE_ALEXNET,IMG_SIZE_ALEXNET))\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    test_data[i][0] = cv2.resize(test_data[i][0],(IMG_SIZE_ALEXNET,IMG_SIZE_ALEXNET))\n",
    "\n",
    "train = train_data[:4000]\n",
    "cv = train_data[4000:]\n",
    "\n",
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE_ALEXNET,IMG_SIZE_ALEXNET,3)\n",
    "Y = np.array([i[1] for i in train])\n",
    "\n",
    "cv_x = np.array([i[0] for i in cv]).reshape(-1,IMG_SIZE_ALEXNET,IMG_SIZE_ALEXNET,3)\n",
    "cv_y = np.array([i[1] for i in cv])\n",
    "test_x = np.array([i[0] for i in test_data]).reshape(-1,IMG_SIZE_ALEXNET,IMG_SIZE_ALEXNET,3)\n",
    "test_y = np.array([i[1] for i in test_data])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(X.shape)\n",
    "\n",
    "print(Y.shape)\n",
    "\n",
    "print(cv_x.shape)\n",
    "\n",
    "print(test_x.shape)\n",
    "\n",
    "steps = len(train)\n",
    "print(steps)\n",
    "remaining = steps % step_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape=[None,IMG_SIZE_ALEXNET,IMG_SIZE_ALEXNET,3])\n",
    "y_true = tf.placeholder(tf.float32,shape=[None,output_classes])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "w_1 = tf.Variable(tf.truncated_normal([11,11,3,96], stddev=0.01))\n",
    "b_1 = tf.Variable(tf.constant(0.0, shape=[[11,11,3,96][3]]))\n",
    "c_1 = tf.nn.conv2d(x, w_1,strides=[1, 4, 4, 1], padding='VALID')\n",
    "c_1 = c_1 + b_1\n",
    "c_1 = tf.nn.relu(c_1)\n",
    "print(c_1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "p_1 = tf.nn.max_pool(c_1, ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1], padding='VALID')\n",
    "print(p_1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "w_2 = tf.Variable(tf.truncated_normal([5,5,96,256], stddev=0.01))\n",
    "\n",
    "b_2 = tf.Variable(tf.constant(1.0, shape=[[5,5,96,256][3]]))\n",
    "c_2 = tf.nn.conv2d(p_1, w_2,strides=[1, 1, 1, 1], padding='SAME')\n",
    "c_2 = c_2 + b_2\n",
    "c_2 = tf.nn.relu(c_2)\n",
    "print(c_2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "p_2 = tf.nn.max_pool(c_2, ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1], padding='VALID')\n",
    "print(p_2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "w_3 = tf.Variable(tf.truncated_normal([3, 3, 256, 384], stddev=0.01))\n",
    "\n",
    "b_3 = tf.Variable(tf.constant(0.0, shape=[[3, 3, 256, 384][3]]))\n",
    "c_3 = tf.nn.conv2d(p_2, w_3,strides=[1, 1, 1, 1], padding='SAME')\n",
    "c_3 = c_3 + b_3\n",
    "c_3 = tf.nn.relu(c_3)\n",
    "print(c_3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "w_4 = tf.Variable(tf.truncated_normal([3, 3, 384, 384], stddev=0.01))\n",
    "b_4 = tf.Variable(tf.constant(0.0, shape=[[3, 3, 384, 384][3]]))\n",
    "c_4 = tf.nn.conv2d(c_3, w_4,strides=[1, 1, 1, 1], padding='SAME')\n",
    "c_4 = c_4 + b_4\n",
    "c_4 = tf.nn.relu(c_4)\n",
    "print(c_4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "w_5 = tf.Variable(tf.truncated_normal([3, 3, 384, 256], stddev=0.01))\n",
    "b_5 = tf.Variable(tf.constant(0.0, shape=[[3, 3, 384, 256][3]]))\n",
    "c_5 = tf.nn.conv2d(c_4, w_5,strides=[1, 1, 1, 1], padding='SAME')\n",
    "c_5 = c_5 + b_5\n",
    "c_5 = tf.nn.relu(c_5)\n",
    "print(c_5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "p_3 = tf.nn.max_pool(c_5, ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1], padding='VALID')\n",
    "print(p_3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "flattened = tf.reshape(p_3,[-1,6*6*256])\n",
    "print(flattened)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_size = int( flattened.get_shape()[1] )\n",
    "w1_fc = tf.Variable(tf.truncated_normal([input_size, nodes_fc1], stddev=0.01))\n",
    "b1_fc = tf.Variable( tf.constant(1.0, shape=[nodes_fc1] ) )\n",
    "s_fc1 = tf.matmul(flattened, w1_fc) + b1_fc\n",
    "s_fc1 = tf.nn.relu(s_fc1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hold_prob1 = tf.placeholder(tf.float32)\n",
    "s_fc1 = tf.nn.dropout(s_fc1,keep_prob=hold_prob1)\n",
    "print(s_fc1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w2_fc = tf.Variable(tf.truncated_normal([nodes_fc1, nodes_fc2], stddev=0.01))\n",
    "b2_fc = tf.Variable( tf.constant(1.0, shape=[nodes_fc2] ) )\n",
    "s_fc2 = tf.matmul(s_fc1, w2_fc) + b2_fc\n",
    "s_fc2 = tf.nn.relu(s_fc2)\n",
    "print(s_fc2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hold_prob2 = tf.placeholder(tf.float32)\n",
    "s_fc2 = tf.nn.dropout(s_fc2,keep_prob=hold_prob1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w3_fc = tf.Variable(tf.truncated_normal([nodes_fc2,output_classes], stddev=0.01))\n",
    "b3_fc = tf.Variable( tf.constant(1.0, shape=[output_classes] ) )\n",
    "y_pred = tf.matmul(s_fc2, w3_fc) + b3_fc\n",
    "print(y_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true,logits=y_pred))\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(cross_entropy)\n",
    "matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "init = tf.global_variables_initializer()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "acc_list = []\n",
    "auc_list = []\n",
    "loss_list = []\n",
    "saver = tf.train.Saver()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(epochs):\n",
    "        for j in range(0,steps-remaining,step_size):\n",
    "            _,c = sess.run([train,cross_entropy],\n",
    "\t\t\tfeed_dict={x:X[j:j+step_size] , y_true:Y[j:j+step_size],hold_prob1:0.5,hold_prob2:0.5})\n",
    "        \n",
    "        \n",
    "\t\tcv_auc_list = []\n",
    "        cv_acc_list = []\n",
    "        cv_loss_list = []\n",
    "        for v in range(0,len(cv_x)-int(len(cv_x) % validating_size),validating_size):\n",
    "            acc_on_cv,loss_on_cv,preds = sess.run([acc,cross_entropy,tf.nn.softmax(y_pred)],\n",
    "\t\t\tfeed_dict={x:cv_x[v:v+validating_size] ,y_true:cv_y[v:v+validating_size] ,hold_prob1:1.0,hold_prob2:1.0})\n",
    "\t\t\t\n",
    "            auc_on_cv = roc_auc_score(cv_y[v:v+validating_size],preds)\n",
    "            cv_acc_list.append(acc_on_cv)\n",
    "            cv_auc_list.append(auc_on_cv)\n",
    "            cv_loss_list.append(loss_on_cv)\n",
    "        acc_cv_ = round(np.mean(cv_acc_list),5)\n",
    "        auc_cv_ = round(np.mean(cv_auc_list),5)\n",
    "        loss_cv_ = round(np.mean(cv_loss_list),5)\n",
    "        acc_list.append(acc_cv_)\n",
    "        auc_list.append(auc_cv_)\n",
    "        loss_list.append(loss_cv_)\n",
    "        print(\"Epoch:\",i,\"Accuracy:\",acc_cv_,\"Loss:\",loss_cv_ ,\"AUC:\",auc_cv_)\n",
    "    \n",
    "    test_auc_list = []\n",
    "    test_acc_list = []\n",
    "    test_loss_list = []\n",
    "    for v in range(0,len(test_x)-int(len(test_x) % validating_size),validating_size):\n",
    "        acc_on_test,loss_on_test,preds = sess.run([acc,cross_entropy,tf.nn.softmax(y_pred)],\n",
    "\t\tfeed_dict={x:test_x[v:v+validating_size] ,y_true:test_y[v:v+validating_size] ,hold_prob1:1.0,hold_prob2:1.0})\n",
    "        \n",
    "        auc_on_test = roc_auc_score(test_y[v:v+validating_size],preds)\n",
    "        test_acc_list.append(acc_on_test)\n",
    "        test_auc_list.append(auc_on_test)\n",
    "        test_loss_list.append(loss_on_test)\n",
    "    saver.save(sess, os.path.join(os.getcwd(),\"CNN_MC.ckpt\"))\n",
    "    test_acc_ = round(np.mean(test_acc_list),5)\n",
    "    test_auc_ = round(np.mean(test_auc_list),5)\n",
    "    test_loss_ = round(np.mean(test_loss_list),5)\n",
    "    print(\"Test Results are below:\")\n",
    "    print(\"Accuracy:\",test_acc_,\"Loss:\",test_loss_,\"AUC:\",test_auc_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f,ax=plt.subplots(1,3,figsize=(12,3))\n",
    "pd.Series(acc_list).plot(kind='line',title='Accuracy on CV data',ax=ax[0])\n",
    "pd.Series(loss_list).plot(kind='line',figsize=(12,7),title='Loss on CV data',ax=ax[1])\n",
    "pd.Series(auc_list).plot(kind='line',figsize=(12,7),title='AUC on CV data',ax=ax[2])\n",
    "plt.subplots_adjust(wspace=0.8)\n",
    "ax[0].set_title('Accuracy on CV data')\n",
    "ax[1].set_title('Loss on CV data')\n",
    "ax[2].set_title('AUC on CV data')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with tf.Session() as session:\n",
    "    saver.restore(session, \"CNN_MC.ckpt\")\n",
    "    print(\"Model restored.\") \n",
    "    print('Initialized')\n",
    "    k = session.run([tf.nn.softmax(y_pred)], feed_dict={x:test_x[0:64] , hold_prob1:1,hold_prob2:1})\n",
    "\n",
    "print(np.array(k).shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "k = np.array(k).reshape(64,output_classes)\n",
    "\n",
    "print(k[0])\n",
    "\n",
    "pred_labels = []\n",
    "\n",
    "for i in range(64):\n",
    "    r = np.round(k[i],3).argmax()\n",
    "    if r ==0 : pred_labels.append(\"glass\")\n",
    "    elif r ==1: pred_labels.append(\"table\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "w=80\n",
    "h=80\n",
    "columns = 8\n",
    "rows = 8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}